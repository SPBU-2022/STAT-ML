{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Тематическое моделирование\n",
    "\n",
    "*Тематическое моделирование* (topic modeling) -- приложение машиннного обучения к анализу текстов.\n",
    "\n",
    "*Тематическая модель* (topic model) коллекции текстовых документов определяет, к каким темам относится каждый документ и какие слова (термины) образуют каждую тему.\n",
    "\n",
    "*Вероятностная тематическая модель* (ВТМ) описывает каждую тему дискретным распределением на множестве терминов, каждый документ дискретным распределением на множестве тем. \n",
    "Предполагается, что коллекция документов -- это последовательность терминов, выбранных случайно и независимо из смеси таких рпспределений, и ставится задача восстановления компонент смеси по выборке.\n",
    "\n",
    "ВТМ осуществляет мягкую классификацию (документ может относится к нескольким выборкам, при этом решается проблема синонимов и омонимов).\n",
    "\n",
    "Применяется для:\n",
    "* выявления трендов в научных публикациях/новостных потоках,\n",
    "* классификации и категоризациии документов, изображений и видео,\n",
    "* информационного поиска, в том числе многоязычного,\n",
    "* тегирования веб-страниц,\n",
    "* обнаружения спама,\n",
    "* рекомендательных систем,\n",
    "* и др."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Вероятностная модель колллекции документов\n",
    "\n",
    "Пусть $D$ -- множество (коллекция) текстовых документов, $W$ -- множество (словарь) всех употребляемых в них терминов (слов или словосочетаний). Каждый документ $d\\in D$ представляет собой последовательность $n_d$ терминов $(w_1,\\dots,w_{n_d})$ из словаря $W$. Термин может повторяться в документе несколько раз.\n",
    "\n",
    "### 7.1.1 Вероятностное пространство и гипотеза независимости \n",
    "\n",
    "Предпологается, что существует конечное множество тем $T$, и каждое употребление термина $w$ в каждом документе $d$ связано с некоторой темой $t\\in T$, которая неизвестна. Коллекция документов рассматривается как множество троек $(d,w,t)$, заданного на конечном множестве $D\\times W \\times T$. Документы $d\\in D$ и термины $w\\in W$ являются наблюдаемыми переменными, тема $t \\in T$ является латентной (скрытой) переменной.\n",
    "\n",
    "Гипотеза о независимости элементов выборки эквивалентна предположению \"Мешок слов\" (bag-of-words), -- что порядок терминов в документах не важен для выявления тематики, то есть тематику документа можно узнать даже после произвольной перестановки терминов, хотя для человека такой текст теряет смысл. Порядок документов в колллекции также не имеет значения -- предположение называют гипотезой \"мешка документов\".\n",
    "\n",
    "Приняв гипотезу \"мешка слов\", можно перейти к более компактному представлению документа как подмножества $d \\subset W$, в ккотором каждому жлементу $w\\in d$ поставлено в соответствие число $n_{dw}$ вхождений термина $w$ в документ $d$.\n",
    "\n",
    "### 7.1.2 Постановка задачи тематического моделирования\n",
    "\n",
    "Построить тематическую модель коллекции документов $D$ -- значит найти множество тем $T$, распределения $p(w|t)$ для всех тем $t\\in T$ и распредления $p(t|d)$ для всех $d \\in D$. Можно также говорить о задаче совместной \"мягкой\" класстеризации множества документов и множества слов по множесту кластер-тем. Мягкая кластеризация означает, что каждый документ или термин не жёстко приписывается какой-то одной теме, а распределяется по нескольким темам.\n",
    "\n",
    "Найденные распределения используются затем для решения прикладных задач. Распределение $p(t|d)$ является удобным признаковым описанием документа в задачах информационного поиска, классификации и категоризации документов.\n",
    "\n",
    "### 7.1.3 Гипотеза условной независимости\n",
    "\n",
    "Будем полагать, что появление слов в документе $d$, относящихся к теме $t$, описывается общим для всей коллекции распредлением $p(w|t)$ и не зависит от документа d. Следующие представления этой гипотезы эквивалентны:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{gathered}\n",
    "    p(w|d,t)=p(w|t); \\\\\n",
    "    p(d|w,t)=p(d|t); \\\\\n",
    "    p(d,w|t)=p(d|t)p(w|t).\n",
    "  \\end{gathered}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.4 Вероятностная модель порождения данных\n",
    "\n",
    "Согласно определению условной вероятности, формуле полной вероятности и гипотезе условной независимости\n",
    "\n",
    "\\begin{equation} \\tag{2}\n",
    "  p(w|d) = \\sum_{t\\in T}p(t|d)p(w|t).\n",
    "\\end{equation}\n",
    "\n",
    "Если формула кажется не очевидной, приведём вывод:\n",
    "\n",
    "Согласно опредлению условной вероятности:\n",
    "$$p(w|d) = \\frac{p(w, d)}{p(d)}$$\n",
    "\n",
    "По формуле полной вероятности:\n",
    "$$p(w,d) = \\sum_{t\\in T} p(w,d|t)p(t)$$\n",
    "\n",
    "По гипотезе условной независимости (1. 3 пункт):\n",
    "$$p(w,d) = \\sum_{t\\in T} p(w|t)p(d|t)p(t)$$\n",
    "\n",
    "Расписав \n",
    "$$p(d|t) = \\frac{p(d,t)}{p(t)},$$\n",
    "получим:\n",
    "$$p(w,d) = \\sum_{t\\in T} p(w|t)p(d,t)$$\n",
    "\n",
    "$$\\scriptsize\\textit{посмотри налево}$$\n",
    "\n",
    "Поскольку $p(d,t)=p(t,d)$,\n",
    "$$p(t,d)=p(t|d)p(d),$$\n",
    "и тогда:\n",
    "$$p(w,d) = \\sum_{t\\in T} p(w|t)p(t|d)p(d).$$\n",
    "\n",
    "Подставив в самое начало, получим: \n",
    "$$p(w|d) = \\frac{\\sum\\limits_{t\\in T} p(w|t)p(t|d)p(d)}{p(d)},$$\n",
    "откуда:\n",
    "$$p(w|d) = \\sum_{t\\in T} p(t|d)p(w|t)p(d).$$\n",
    "\n",
    "Если распределения $p(t|d)$ и $p(w|t)$ известны, то вероятностная модель $(2)$ описывает процесс порождения коллекции $D$. \n",
    "\n",
    "Построение тематической модели -- обратная задача: по известной коллекции $D$ требуется восстановить породившие её распределения $p(t|d)$ и $p(w|t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ALGORITHM](Алгоритм.bmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.5 Гипотеза разреженности\n",
    "\n",
    "Естественно предполагать, что каждый документ $d$ и каждый термин $w$ связан с небольшим числом тем $t$. В таком случае значительная часть вероятностей $p(t|d)$ и $p(w|t)$ должна обращаться в ноль.\n",
    "\n",
    "Если документ относится к большому числу тем, то в задачах тематического поиска или классификации документов его имеет смысл разбивать на более однородные по тематике части.\n",
    "\n",
    "Если термин относится к большому числу тем, то, скорее всего, это общеупотребительное слово, бесполезное для определение тематики.\n",
    "\n",
    "Алгоритмы, в которых нулевые значения не хранятся, намного эффективнее по памяти и по скорости. Поэтому для больших коллекций разреженнность должна учитываться обязательно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.6 Частотные оценки условных вероятностей\n",
    "\n",
    "Вероятности, связанные с наблюдаемыми перменными $d$ и $w$, можно оценивать по выборке как частоты:\n",
    "\n",
    "\\begin{equation} \\tag{3}\n",
    "  \\hat{p}(d, w)=\\frac{n_{dw}}{n}, \\quad \\hat{p}(d)=\\frac{n_d}{n},\\quad  \\hat{p}(w)=\\frac{n_w}{n}, \\quad \\hat{p}(w|d)=\\frac{n_{dw}}{n_d},\n",
    "\\end{equation}\n",
    "\n",
    "где\n",
    "* $n_{dw}$ -- число вхождений термина $w$ в документ $d$,\n",
    "* $n_d=\\sum\\limits_{w\\in W}n_{dw}$ -- длина документа $d$ в терминах,\n",
    "* $n_w = \\sum\\limits_{d\\in D}n_{dw}$ -- число вхождений термина $w$ во все документы коллекции,\n",
    "* $n=\\sum\\limits_{d\\in D} \\sum\\limits_{w\\in d}n_{dw}$ -- длина коллекции в терминах.\n",
    "\n",
    "Вероятности, связанные со скрытой переменной $t$, также можно оценивать как частоты, если рассматривать коллекцию документов как выборку троек $(d,w,t)$:\n",
    "\n",
    "\\begin{equation} \\tag{4}\n",
    "  \\hat{p}(t)=\\frac{n_{t}}{n}, \\quad \\hat{p}(w|t)=\\frac{n_{wt}}{n_t}, \\quad \\hat{p}(t|d)=\\frac{n_{dt}}{n_d}, \\quad \\hat{p}(t|d,w)=\\frac{n_{dwt}}{n_{dw}},\n",
    "\\end{equation}\n",
    "\n",
    "где\n",
    "* $n_{dwt}$ -- число троек, в которых термин $w$ документа $d$ связан с темой $t$, \n",
    "* $n_{dt}=\\sum\\limits_{w\\in W}n_{dwt}$ -- число троек, в которых термин документа $d$ связан с темой $t$,\n",
    "* $n_{wt} = \\sum\\limits_{d\\in D}n_{dwt}$ -- число троек, в которых термин $w$ связан с темой $t$,\n",
    "* $n_t=\\sum\\limits_{d\\in D} \\sum\\limits_{w\\in d}n_{dw}$ -- число троек, связанных с темой $t$.\n",
    "\n",
    "По ЗБЧ в пределе при $n\\to\\infty$ частотные оценки $\\hat{p}(\\cdot)$, определяемые формулами $3$, $4$, стремятся к соответствующим вероятностям $p(\\cdot)$. Частотная интерпретация даёт ясное понимание всех условных вероятностей, которые будут использоваться в дальнейшем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.7 Стохастическое матричное разложение\n",
    "\n",
    "Если число тем $|T|$ много меньше числа документов $|D|$ и числа терминов $|W|$, то равенство $(2)$ можно понимать как задачу приближённого представления заданной матрицы частот\n",
    "\n",
    "$$\n",
    "  \\mathbb{F}=(\\hat{p}_{wd})_{W\\times D}, \\quad \\hat{p}_wd=\\hat{p}(w|d) = n_{dw}/n_{d},\n",
    "$$\n",
    "в виде произведения $\\mathbb{F}\\approx \\Phi\\Theta$ двух неизвестных матриц меньшего размера -- матрицы терминов тем $\\Phi$ и матрицы тем документов $\\Theta$:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "  \\Phi = (\\varphi_{wt})_{W\\times T}, \\quad \\varphi_{wt}=p(w|t); \\\\\n",
    "  \\Theta = (\\theta_{td})_{T\\times D}, \\quad \\theta_{td}=p(t|d).\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "Матрицы, столбцы которых неотрицательны и нормированны, называются стохастическими. \n",
    "\n",
    "Представление матрицы $\\mathbb{F}$ получается с помощью принципа максимума правдоподобия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.8 Принцип максимума правдоподобия.\n",
    "\n",
    "Для оценивания параметров $\\Phi$, $\\Theta$ тематической модели по коллекции документов $D$ будем максимизировать правдоподобие (плотность распределения) выборки:\n",
    "\n",
    "\\begin{equation} \\tag{6}\n",
    "p(D;\\Phi, \\Theta) = C \\prod\\limits_{d\\in D}\\prod\\limits_{w\\in d}p(d, w)^{n_{dw}} = \\prod\\limits_{d\\in D}\\prod\\limits_{w\\in d} p(w|d)^{n_{dw}} Cp(d)^{n_{dw}} \\to \\max\\limits_{\\Phi, \\Theta},\n",
    "\\end{equation}\n",
    "где $С$ -- нормировочный множитель, зависящий от чисел $n_{dw}$. Отбросим постоянную часть:\n",
    "\n",
    "$$\n",
    "\\tilde{p}(D;\\Phi, \\Theta) = \\prod\\limits_{d\\in D}\\prod\\limits_{w\\in d} p(w|d)^{n_{dw}} \\to \\max\\limits_{\\Phi, \\Theta}.\n",
    "$$\n",
    "\n",
    "Подставим выражение для $p(w|d)$ из $(2)$:\n",
    "\n",
    "$$\n",
    "\\tilde{p}(D;\\Phi, \\Theta) = \\prod\\limits_{d\\in D}\\prod\\limits_{w\\in d} \\left(\\sum_{t\\in T}p(t|d)p(w|t)\\right)^{n_{dw}} \\to \\max\\limits_{\\Phi, \\Theta}.\n",
    "$$\n",
    "\n",
    "Обозначим $\\theta_{td} = p(t|d)$, $\\varphi_{wt} = p(w|t)$:\n",
    "\n",
    "$$\n",
    "\\tilde{p}(D;\\Phi, \\Theta) = \\prod\\limits_{d\\in D}\\prod\\limits_{w\\in d} \\left(\\sum_{t\\in T}\\theta_{td} \\varphi_{wt}\\right)^{n_{dw}} \\to \\max\\limits_{\\Phi, \\Theta}.\n",
    "$$\n",
    "\n",
    "Логарифмируем:\n",
    "\n",
    "$$\n",
    "L(\\Phi, \\Theta) = \\ln{\\tilde{p}(D;\\Phi, \\Theta)} = \\ln{\\left(\\prod\\limits_{d\\in D}\\prod\\limits_{w\\in d} \\left(\\sum_{t\\in T}\\theta_{td} \\varphi_{wt}\\right)^{n_{dw}}\\right)} = \n",
    "\\sum\\limits_{d\\in D}\\sum\\limits_{w\\in d} n_{dw} \\ln{\\sum_{t\\in T}\\theta_{td} \\varphi_{wt}} \\to \\max\\limits_{\\Phi, \\Theta}.\n",
    "$$\n",
    "\n",
    "Получили задачу максимизации логарифма правдоподобия при ограничениях неотрицательности и нормированности стобцов матриц $\\Phi$ и $\\Theta$:\n",
    "\n",
    "\\begin{equation} \\tag{7}\n",
    "  \\begin{gathered}\n",
    "    L(\\Phi, \\Theta) = \\sum\\limits_{d\\in D}\\sum\\limits_{w\\in d} n_{dw} \\ln{\\sum_{t\\in T}\\theta_{td} \\varphi_{wt}} \\to \\max\\limits_{\\Phi, \\Theta}, \\\\\n",
    "    s.t. \\\\\n",
    "    \\sum\\limits_{w\\in W} \\varphi_{wt}=1; \\quad \\varphi_{wt} \\geqslant 0, \\\\\n",
    "    \\sum\\limits_{t\\in T} \\theta_{td}=1; \\quad \\theta_{td} \\geqslant 0. \\\\\n",
    "  \\end{gathered}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Предварительная обработка текстовых данных\n",
    "\n",
    "\n",
    "\n",
    "При построении тематической модели нет смысла различать формы (склонения, спряжения) одного и того же слова. Это приведёт к неоправданному разрастанию словаря, дроблению статистики, увеличению ресурсоёмкости и снижению качества модели.\n",
    "\n",
    "### 7.2.1 Лемматизация и стемминг\n",
    "\n",
    "*Лемматизация* -- приведение каждого слова в документе к нормальной форме. \n",
    "\n",
    "В русском:\n",
    "* Существительные: именительный падеж, единственное число;\n",
    "* Прилагательные: именительный падеж, единственное число, мужской род;\n",
    "* Глагол, причастие, деепричастие: глагол в инфинитиве.\n",
    "\n",
    "Лемматизаторы разрабатываются с помощью составления грамматического словаря со всеми формами слов, либо аккуратной формализации правил языка со всеми исключениями. Недостатками лемматизаторов является неполнота словарей, особенно по части специальной терминологии и неологизмов, которые часто и представляют наибольший интерес.\n",
    "\n",
    "*Стемминг* -- отбрасывание изменяемых частей слов, в основном, окончаний.\n",
    "\n",
    "Плюсы: \n",
    "* Не требует словаря;\n",
    "* Основана на правилах морфологии языка;\n",
    "* Хорошо работает с английским языком;\n",
    "\n",
    "Минусы:\n",
    "* Большое число ошибок;\n",
    "* Плохо работает для русского языка;\n",
    "\n",
    "### 7.2.2 Отбрасывание стоп-слов\n",
    "\n",
    "Слова, которые встречаются во многих текстах различной тематики -- бесполезны и могут быть отброшены. Это союзы, предлоги, числительные, местоимения, некоторые глаголы, прилагательные и наречия. Отбрасывание почти не влияет на длину словаря, но может приводить к заметному сокращению длины некоторых текстов.\n",
    "\n",
    "### 7.2.3 Отбрасывание редких слов\n",
    "\n",
    "Слова, встречающие в длинном документе слишком редко, например, только один раз, можно отбросить, полагая, что данное слово не характеризует тематику данного документа.\n",
    "\n",
    "### 7.2.4 Выделение ключевых фраз\n",
    "\n",
    "При обработке специальных текстов вместо отдельных слов выделяют *ключевые фразы* -- словосочетания, являющиеся терминами предметной области. Это отдельная сложная задача, для решения которой необходимо привлечение экспертов (даже при использовании методов машинного обучения).\n",
    "\n",
    "***\n",
    "\n",
    "Далее будем полагать, что словарь $W$ получен в результате предварительной обработки всех документов коллекции $D$ и может содержать как отдельные слова, так и ключевые фразы. Элементы словаря $w\\in W$ будем называть \"терминами\".\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Вероятностный латентный сематический анализ (PLSA)\n",
    "\n",
    "Вероятностный латентный семантический анализ (probabilistic latent semantic analysis).\n",
    "\n",
    "Вероятностная модель появления пары \"документ-термин\" $(d,w)$ записывается тремя эквивалентными способами:\n",
    "\n",
    "$$\n",
    "p(d,w) = \\sum\\limits_{t\\in T}p(t)p(w|t)p(d|t) = \\sum\\limits_{t\\in T}p(d)p(w|t)p(t|d) = \\sum\\limits_{t\\in T}p(w)p(t|w)p(d|t),\n",
    "$$\n",
    "где $p(t)$ -- распределение тем во всей коллекции. Первое представление называется симметричным, остальные -- несимметричными. Они приводят к немного разным итерационным процессам обучения тематической модели.\n",
    "\n",
    "Сейчас возьмём второе представление, совпадающее с $(2)$.\n",
    "\n",
    "### 7.3.1 EM-алгоритм\n",
    "\n",
    "Для решение задачи $(6)$ в $\\mathrm{PLSA}$ применяется итерационный процесс, в котором каждая итерация состоит из двух шагов $\\mathrm{E}$ и $\\mathrm{M}$. Перед первой итерацией выбирается начальное приближение параметров $\\varphi_{wt}$ и $\\theta_{td}$.\n",
    "\n",
    "На $\\mathrm{E}$-шаге по текущим значениям параметров $\\varphi_{wt}$ и $\\theta_{td}$ с помощью формулы Байеса вычисляются условные вероятности $p(t|d,w)$ всех тем $t\\in T$ для каждого термина $w\\in d$ в каждом документе $d$:\n",
    "\n",
    "\\begin{equation} \\tag{8}\n",
    "  H_{dwt}=p(t|d,w)=\\frac{p(w|t)p(t|d)}{p(w|d)} = \\frac{\\varphi_{wt}\\theta_{td}}{\\sum\\limits_{s\\in T}\\varphi_{ws}\\theta_{sd}}\n",
    "\\end{equation}\n",
    "\n",
    "На $\\mathrm{M}$-шаге, наоборот, по условным вероятностям тем $H_{dwt}$ вычисляется новое приближение параметров $\\varphi_{wt}$, $\\theta_{td}$. Поскольку\n",
    "\n",
    "\\begin{equation} \\tag{9}\n",
    "  \\hat{n}_{dwt}=n_{dw}p(t|d,w)=n_{dw}H_{dwt}\n",
    "\\end{equation}\n",
    "оценивает (не обязательно целое) число $n_{dwt}$ вхождений термина $w$ в документ $d$, связанных с темой $t$. Просуммировав $\\hat{n}_{dwt}$ по документам $d$ и по терминам $w$, получим оценки $\\hat{n}_{wt}, \\hat{n}_{dt}, \\hat{n}_t$ и через них, согласно $(4)$ -- частотные оценки условных вероятностей $\\varphi_{wt}$, $\\theta_{td}$:\n",
    "\n",
    "\\begin{equation} \\tag{10}\n",
    "  \\varphi_{wt}=\\frac{\\hat{n}_{wt}}{\\hat{n}_t}, \\quad \n",
    "  \\hat{n}_t = \\sum\\limits_{w\\in W} \\hat{n}_{wt}, \\quad\n",
    "  \\hat{n}_{wt} = \\sum\\limits{d\\in D} n_{dw} H_{dwt}.\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation} \\tag{11}\n",
    "  \\theta_{td}=\\frac{\\hat{n}_{dt}}{\\hat{n}_d}, \\quad \n",
    "  \\hat{n}_d = \\sum\\limits_{t\\in T} \\hat{n}_{dt}, \\quad\n",
    "  \\hat{n}_{dt} = \\sum\\limits{w\\in d} n_{dw} H_{dwt}.\n",
    "\\end{equation}\n",
    "\n",
    "Покажем, что эти оценки действительно решают задачи $(6)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем лагранжиан задачи $(6)$ при ограничениях нормировки, но проигноривовав ограничения неотрицательности (позже убедимся, что решение неотрицательно):\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "  \\mathscr{L}(\\Phi, \\Theta) = \\sum\\limits_{d \\in D} \\sum\\limits_{w \\in d} n_{dw} \\ln{\\sum\\limits_{t \\in T}\\varphi_{wt}\\theta_{td}}-\\sum\\limits_{t\\in T}\\lambda_t (\\sum\\limits_{w\\in W}\\varphi_{wt}-1) - \\sum\\limits_{d\\in D}\\mu_d (\\sum\\limits_{t\\in T}\\theta_{td}-1) = \\\\\n",
    "  =\\sum\\limits_{d \\in D} \\sum\\limits_{w \\in d} n_{dw} \\ln{p(w|d)}-\\sum\\limits_{t\\in T}\\lambda_t (\\sum\\limits_{w\\in W}\\varphi_{wt}-1) - \\sum\\limits_{d\\in D}\\mu_d (\\sum\\limits_{t\\in T}\\theta_{td}-1). \n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "Продифференцировав лагранжиан по $\\varphi_{wt}$ и приравняв к нулю производную, получим\n",
    "\n",
    "\\begin{equation} \\tag{12}\n",
    "  \\lambda_t = \\sum\\limits_{d \\in D} n_{dw} \\frac{\\theta_{td}}{p(w|d)}.\n",
    "\\end{equation}\n",
    "\n",
    "Домножим обе части этого равенства на $\\varphi_{wt}$, просуммируем по всем терминам $w\\in W$, применим услование нормировки вероятностей $\\varphi_{wt}$ в левой части и выделим переменную $H_{dwt}$ в правой части. Получим\n",
    "$$\n",
    "  \\lambda_t = \\sum\\limits_{d\\in D}\\sum_{w \\in W} n_{dw}H_{dwt}.\n",
    "$$\n",
    "\n",
    "Снова домножим обе части $(12)$ на $\\varphi_{wt}$, выделим переменную $H_{dwt}$ в правой части и выразим $\\varphi_{wt}$ из левой части, подставим уже известное выражение для $\\lambda_t$. Получим\n",
    "$$\n",
    "\\varphi_{wt} = \\frac{\\sum\\limits_{d\\in D}n_{dw}H_{dwt}}{\\sum\\limits_{w^\\prime \\in W}\\sum\\limits_{d\\in D} n_{dw^\\prime}H_{dwt}}.\n",
    "$$\n",
    "\n",
    "Преобразуем:\n",
    "\n",
    "$$\n",
    "  \\varphi_{wt} = \n",
    "  \\frac{\\sum\\limits_{d\\in D}n_{dw}H_{dwt}}{\\sum\\limits_{w^\\prime \\in W}\\sum\\limits_{d\\in D} n_{dw^\\prime}H_{dwt}} = \n",
    "  \\frac{\\sum\\limits_{d\\in D}\\hat{n}_{dwt}}{\\sum\\limits_{w^\\prime \\in W}\\sum\\limits_{d\\in D} \\hat{n}_{dw^\\prime t}} = \n",
    "  \\frac{\\hat{n}_{wt}}{\\sum\\limits_{w^\\prime \\in W}\\sum\\limits_{d\\in D} \\hat{n}_{dw^\\prime t}} = \n",
    "  \\frac{\\hat{n}_{wt}}{\\hat{n}_t}.\n",
    "$$\n",
    "\n",
    "Получили $(10)$. Проделав аналогичные действия с производной лагранжиана по $\\theta_{td}$, получим $(11)$.\n",
    "\n",
    "Если начальные приближения $\\theta_{td}$ и $\\varphi_{wt}$ положительны, то и после каждой итерации они будут оставаться положительными, несмотря на то, что ограничение неотрицательности было проигнорировано в ходе решения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2 Эффективность $\\mathrm{EM}$-алгоритма \n",
    "\n",
    "Число операций -- $O(n|T|I)$, где $n$ -- длина коллекции $|T|$ -- число тем, $I$ -- число итераций.\n",
    "\n",
    "Перевбор всех терминов $w$ во всех документах $d$ можно организовать очень эффективно, если хранить каждый документ $d$  в виде последовательности пар $(w, n_{dw})$.\n",
    "\n",
    "### 7.3.3 Рациональный $\\mathrm{EM}$-алгоритм\n",
    "\n",
    "Вычисление переменных $\\hat{n}_{wt}$, $\\hat{n}_{dt}$, $\\hat{n}_{t}$ на $\\mathrm{M}$-шаге требует однократного прохода всей коллекции в цикле по всем документам $d\\in D$ и всем терминами $w \\in d$. Внутри этого цикла переменные $H_{dwt}$ можно вычислять непосредственно в тот момент, когда они понадобятся. От этого результат алгоритма не изменяется, $\\mathrm{E}$-шаг встраивается внутрь $\\mathrm{M}$-шага без дополонительных вычислительных затрат, отпадает необходимость хранения трёхмерной матрицы $H_{dwt}$. Заметим также, что переменную $\\hat{n}_d$ можно не вычислять, поскольку $\\hat{n}_d=n_d$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PLSA_EM](PLSA_EM_Alg.bmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.4 Обобщённый $\\mathrm{EM}$-алгоритм\n",
    "\n",
    "Поскольку функционал правдопободия известен не точно, он зависит от приближённых значений $H_{dwt}$, полученных на $\\mathrm{E}$-шаге, нет необходимости сверхточно решать задачу максимизации на $\\mathrm{M}$-шаге, достаточно ещё немного приблизиться к точке максимума правдоподобия и снова выполнить $\\mathrm{E}$-шаг. \n",
    "\n",
    "В обобщённом $\\mathrm{EM}$-алгоритме (generalized $\\mathrm{EM}$-algorithm, GEM) сокращённый $\\mathrm{M}$-шаг. \n",
    "\n",
    "В другом обобщении $\\mathrm{E}$-шаг выполняется для части скрытых переменных $H_{dwt}$. После этого $\\mathrm{M}$ выполняется только для тех основных переменных $\\varphi_{wt}$, $\\theta_{td}$, которые зависят от изменившихся скрытых переменных. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PLSA_GEM](PLSA_GEM_Alg.bmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сокращение $\\mathrm{M}$-шага сводится к более частому обновлению параметров $\\theta_{td}$ и $\\varphi_{wt}$.\n",
    "\n",
    "Параметры $\\Phi$, $\\Theta$ пора обновить, когда:\n",
    "* После каждого прохода коллекции; (тогда не суть не поменяется, а это медленно)\n",
    "* После каждого документа;\n",
    "* После каждого термина $(d, w)$;\n",
    "* После заданного числа терминов;\n",
    "* После каждого вхождения термина;\n",
    "\n",
    "На больших коллекциях частые обновления повышают скорость сходимости и почти не влияют на результат. Отсюда практическая рекомендация дделать обновления после каждого термина, при этом каждый термин документа обрабатывается только один раз. Этот способ позволяет ещё отказаться от матриц $\\Theta$ и $\\Phi$, поскольку значения $\\theta_{td}$ и $\\varphi_{wt}$ можно вычислять \"на лету\".\n",
    "\n",
    "При первом проходе коллекции частые обновления не делаются, чтобы в счётчиках накопилась информация по всей коллекции. В противном случае оценки параметров $\\theta_{td}$ и $\\varphi_{wt}$ по начальному фрагменту выборки могут оказаться хуже начального приближения. Начиная со второй итерации для каждой пары $(d,w)$ из счётчиков $\\hat{n}_{wt}$ и $\\hat{n}_{dt}$ вычисляется $n_{dwt}$ -- то самое $\\delta$, которое было к ним прибавлено при обработке пары $(d,w)$ на предыдущей итерации. Т.о., счётчики $\\hat{n}_{wt}$ и $\\hat{n}_{dt}$ всегда содержат результат последнего однократного прохода всей матрицы.\n",
    "\n",
    "Необходимость хранения трёхмерной матрицы $n_{dwt}$ делает этот алгоритм неприменимым к большим коллекциям. Это можно устранить путём реорганизации итераций или применением сэмплирования. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Начальные приближения\n",
    "\n",
    "Начальные приближения $\\varphi_{t}$ и $\\theta_{d}$ можно задавать нормированными случайными векторами из равномерного распределения.\n",
    "\n",
    "Другая распространённая рекомендация -- пройти по всей коллекции, выбрать для каждой пары (d, w) случайную тему $t$ и вычислить частотные оценки $(4)$ вероятностей $\\varphi_{wt}$ и $\\theta_{td}$ для всех $d \\in D$, $w \\in W$, $t \\in T$.\n",
    "\n",
    "### 7.4.1 Инициализация с частичным обучением\n",
    "Применяется в случаях, когда темы известны заранее и имеются дополнительные данные о привязке некоторых документов или терминов к темам. Учёт этих данных улучшает интерпретируемость тем.\n",
    "\n",
    "Если известно, что документ $d$ относится к подмножеству тем $T_d \\subset T$, то в качестве начального $\\theta_{td}$ можно взять равномерное распределение на этом подмножестве:\n",
    "\n",
    "\\begin{equation} \\tag{13}\n",
    "  \\theta^0_{td} = \\frac{1}{|T_d|}[t\\in T_d].\n",
    "\\end{equation}\n",
    "\n",
    "Если известно, что подмножество терминов $W_t \\subset W$ относится к теме $t$, то в качестве начального $\\varphi_{wt}$ можно взять равномерное распределение на $W_t$:\n",
    "\n",
    "\\begin{equation} \\tag{14}\n",
    "  \\varphi_{wt} = \\frac{1}{|W_t|}[w\\in W_t].\n",
    "\\end{equation}\n",
    "\n",
    "Если известно, что подмножество документов $D_t \\subset D$ относится к теме $t$, то можно взять эмпирическое распределение слов в объединённом документе:\n",
    "\n",
    "\\begin{equation} \\tag{15}\n",
    "  \\theta^0_{wt} = \\frac{\\sum\\limits_{d\\in D_t} n_{dw}}{\\sum\\limits_{d\\in D_t} n_d}.\n",
    "\\end{equation}\n",
    "\n",
    "Если нет никакой априорной информации о связи документов с темами, то последнюю формулу можно применить к случайным подмножествам документов $D_t$, как вариант -- предлагается брать один случайный документ.\n",
    "\n",
    "### 7.4.2 Инициализация $\\Theta$ по $\\Phi$\n",
    "\n",
    "Если для всех тем известны начальные приближения $\\varphi^0_{wt}$, то первая итерация $\\mathrm{ЕМ}$-алгоритма при равномерном распределении $\\theta^0_{td} = 1/|T|$ даёт ещё одну интуитивно очевидную формулу инициализации:\n",
    "\n",
    "\\begin{equation} \\tag{16}\n",
    "  \\theta_{td} = \\frac{1}{n_d}\\sum\\limits_{w\\in d}n_{dw}H_{dwt}=\\sum\\limits_{w\\in d}\\frac{n_{dw}}{n_d}\\frac{\\varphi_{wt}}{\\sum\\limits_{s}\\varphi_{ws}} = \\sum\\limits_{w\\in d} \\hat{p}(w|d)\\hat{p}(t|w).\n",
    "\\end{equation}\n",
    "\n",
    "Здесь распределение тем в документе $d$ оценивается путём усреднения распределений тем $p(t|w)$ по словам документа $d$, вычисленных по формуле Байеса.\n",
    "\n",
    "### 7.4.3 Недостатки PLSA\n",
    "\n",
    "* Слишком много параметров $\\varphi_{wt}$ и $\\theta_{td}$: $(|W||T|+|T||D|)$.\n",
    "* Неверно оценивает вероятность новых слов ($\\hat{p}(w|t) = 0$ для слова, которого не было в обучающейся коллекции, но оно встретилось в каком-нибудь документе).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 7.5 Дивергенция Кульбака-Лейблера (или `KL`-дивергенция)\n",
    "\n",
    "`KL`-дивергенция между дискретными распределениями $P=(p_i)^n_{i=1}$ и $Q=(q_i)^n_{i=1}$ -- несимметричная функция расстояния (и поэтому называть её фунцией расстояния -- некорректно):\n",
    "$$\n",
    "\\mathrm{KL}(P||Q) \\equiv \\mathrm{KL}_i(p_i || q_i) = \\sum\\limits^{n}_{i=1}p_i\\ln{\\frac{p_i}{q_i}}.\n",
    "$$\n",
    "\n",
    "Предполагается, что $p_i>0$ и $q_i>0$. `KL`-дивергенция не является вполне адекватной функцией расстояния, когда у распределений $P$ и $Q$ не совпадают носители $\\Omega_P={i: p_i>0}$ и $\\Omega_Q={i: q_i>0}$.\n",
    "\n",
    "Наиболее важные свойства:\n",
    "1. Неотрицательна. Если $\\Omega_P = \\Omega_Q$, то $\\mathrm{KL}(P||Q)=\\mathrm{KL}(Q||P)=0 \\Leftrightarrow p_i=q_i$ (когда распределения совпадают).\n",
    "2. Является мерой вложенности распределений. Если $\\mathrm{KL}(P||Q)<\\mathrm{KL}(Q||P)$, то распределение $P$ сильнее вложено в $Q$, чем $Q$ в $P$.\n",
    "![KL](KL.bmp)\n",
    "3. Если $P$ -- эмпирическая функция распределения, а $Q(\\alpha)$ параметрическое семейство (модель) распределений, то минимизация `KL`-дивергенции эквивалентна максимизации правдоподобия:\n",
    "$$\n",
    "\\mathrm{KL}(P||Q(\\alpha))=\\sum\\limits^n_{i=1} p_i \\ln{\\frac{p_i}{q_i(\\alpha)}}\\to \\min_a \\Leftrightarrow \\sum\\limits^n_{i=1} p_i\\ln{q_i(\\alpha)}\\to \\max_\\alpha\n",
    "$$\n",
    "\n",
    "Максимизация правдоподобия $(6)$ эквивалентна минимизации взвешенной суммы дивергенций Кульбака-Лейблера между эмпирическими распределениями $\\hat{p}(w|d)=n_{dw}/n_d$ и модельными $p(w|d)$, по всем документам $d\\in D$:\n",
    "$$\n",
    "  \\sum\\limits_{d\\in D} n_d\\mathrm{KL}_w \\left( \\frac{n_{dw}}{n_d} \\big | \\big | \\sum\\limits_{t\\in T} \\varphi_{wt}\\theta_{td} \\right) \\to \\min_{\\Phi, \\Theta},\n",
    "$$\n",
    "где весом документа $d$ явялется его длина $n_d$. Если веса $n_d$ убрать, то все документы будут искусственно приведены к одинаковой длине. Такая модификация функционала качества может быть полезна при моделировании коллекций,содержащих документы одинаковой важности, но существенно разной длины.\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c51ecd44652e2a4b2ee134d77872454324aad32df1ae4617bbc3dbb8ead856d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
